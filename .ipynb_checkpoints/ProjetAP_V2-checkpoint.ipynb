{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5e381b",
   "metadata": {},
   "source": [
    "## Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65c3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412cff41-793f-46f0-a2d6-f196dc2c61d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8ba82-af8f-41e0-b2de-caaf92b39769",
   "metadata": {},
   "source": [
    "<h3> Parametrage des Paths </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b6d01f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dacd7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_trainingA=data_path+'training_setA/'\n",
    "path_trainingB=data_path+'training_setB/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6596138-ffeb-42ea-a3b1-bf1661e11217",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0440ce2-c268-4eda-8f23-689b19b969d7",
   "metadata": {},
   "source": [
    "<h3> Implémentation des méthodes utilisées </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ef625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psv_to_dataframe(fname):\n",
    "        \"\"\"Transforme un fichier patient en un psv avec un identifiant\n",
    "        utilisé pour concaténer les données sans perdre la dépendance \n",
    "        examen/patient spécifique\"\"\"\n",
    "        df = pd.read_csv(fname, sep='|')\n",
    "        df['id'] = int(fname.split('.psv')[0].split('p')[-1])\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b762be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Méthode qui permet de fill les NaN avec une méthode qui s'appel le ForwardFill\n",
    "et qui fait aussi du Backward fill\"\"\"\n",
    "\n",
    "def impute_missing_vals(df, attributes):\n",
    "    df_clean = df.copy()\n",
    "    for att in attributes:\n",
    "        if df_clean[att].isnull().sum() == len(df_clean):\n",
    "            df_clean[att] = df_clean[att].fillna(0) # On remplie les cases par des 0\n",
    "        elif df_clean[att].isnull().sum() == len(df_clean) - 1:\n",
    "            df_clean[att] = df_clean[att].ffill().bfill() # On remplie les cases à l'aide d'un Forward Fill et d'un Backward Fill\n",
    "        else:\n",
    "            df_clean[att] = df_clean[att].interpolate(method='nearest', limit_direction='both') # On utilise une méthode d'interpolation pour remplir avec des valeurs non nulles.\n",
    "            df_clean[att] = df_clean[att].ffill().bfill()\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a62293b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on parcours le training set A et on créer la liste des dataframes des patients\n",
    "#qu'on incorpore dans une liste et on créer un objet sérializer de celui-ci\n",
    "def createDataframeA():\n",
    "    patient_individuel_A = []\n",
    "    for p in patient_id_A:\n",
    "        df = pd.read_csv(path_trainingA+'/'+p,sep='|')\n",
    "        attributes = df.columns[:-1]\n",
    "        df_clean = impute_missing_vals(df,attributes)\n",
    "        df_clean = df_clean.drop(['Unit1', 'Unit2', 'EtCO2'], axis=1)\n",
    "        patient_individuel_A.append(df_clean)\n",
    "    with open(\"list_individu_A\",\"wb\") as outfile: # On créer un objet sérialisé de la liste de chaque patient.\n",
    "        pickle.dump(patient_individuel_A,outfile)\n",
    "        return patient_individuel_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a91bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on parcours le training set B et on créer la liste des dataframes des patients\n",
    "#qu'on incorpore dans une liste et on créer un objet sérializer de celui-ci\n",
    "def createDataframeB():\n",
    "    patient_individuel_B = []\n",
    "    for p in patient_id_B:\n",
    "        df = pd.read_csv(path_trainingB+'/'+p,sep='|')\n",
    "        attributes = df.columns[:-1]\n",
    "        df_clean = impute_missing_vals(df,attributes)\n",
    "        df_clean = df_clean.drop(['Unit1', 'Unit2', 'EtCO2'], axis=1)\n",
    "        patient_individuel_B.append(df_clean)\n",
    "    with open(\"list_individu_A\",\"wb\") as outfile: # On créer un objet sérialisé de la liste de chaque individu.\n",
    "        pickle.dump(patient_individuel_A,outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d5027-2e59-48df-9799-bc483b6bb4c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65321cc-6243-482f-9275-70edbcf2d433",
   "metadata": {},
   "source": [
    "<h3> Création des Dataframes </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815e2c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(path_trainingA) and os.path.exists(path_trainingB):  # Si le chemin vers les deux training sets existe\n",
    "    patient_id_A = sorted(os.listdir(path_trainingA)) # on trie le set A\n",
    "    patient_id_B = sorted(os.listdir(path_trainingB)) # on trie le set B\n",
    "    patient_individuel_A = createDataframeA() # on créer le dataframe pour le set A\n",
    "    patient_individuel_B = createDataframeB() # on créer le dataframe pour le set B\n",
    "elif os.path.exists(\"./list_individu_B\") and os.path.exists(\"./list_individu_A\"):  # Si les fichiers sérialisés existent, on les importe.  \n",
    "    with open(\"list_individu_A\",\"rb\") as infile:\n",
    "        list_A = pickle.load(infile)\n",
    "    with open(\"list_individu_B\",\"rb\") as infile:\n",
    "        list_B = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d0daf4",
   "metadata": {},
   "source": [
    "Les cellules ci dessous permettent d'avoir une liste des dataframes de l'ensemble des patients individuellement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf58527",
   "metadata": {},
   "source": [
    "Les cellules en dessous permettent d'avoir un Dataframe contenant l'intégralité des données patient en 1 seule fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfbee26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_path+'allA') and os.path.exists(path_trainingA): # On vérifie si le chemin vers le fichier sérialisé existe \n",
    "    concat_patientA = [] # on créer une liste qui va servir à contenir les patients.\n",
    "    for p in patient_id_A:  # Pour chaque patient dans le training set A, on récupère le PSV, rempli les valeurs NaN, enlève la colonne Unit1, Unit 2 et EtCO2.\n",
    "        df = psv_to_dataframe(path_trainingA+'/'+p)\n",
    "        attributes = df.columns[:-2]\n",
    "        df_clean = impute_missing_vals(df,attributes)\n",
    "        df_clean = df_clean.drop(['Unit1', 'Unit2', 'EtCO2'], axis=1)\n",
    "        concat_patientA.append(df_clean) # On ajoute le patient clean à la liste des patients.\n",
    "    df_allA = pd.concat(concat_patientA) # On créer un Dataframe de contenant tous les patients et donc tout leur examens.\n",
    "    df_allA.to_csv(data_path+'allA') # On créer un fichier de ce dataFrame\n",
    "elif os.path.exists(data_path+'allA'): # Si le chemin vers le fichier CSV contenant tout les patients existe, on le récupère.\n",
    "    df_allA = pd.read_csv(data_path+'allA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "525f78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_path+'/allB') and os.path.exists(path_trainingB): # On vérifie si le chemin vers les fichierds de données existe.\n",
    "    concat_patientB = [] # on créer une liste qui va servir à contenir les patients\n",
    "    for p in patient_id_B: # On effectue la même manipulation des données qu'auparavant, enlevant Unit1, Unit2, EtCO2.\n",
    "        df = psv_to_dataframe(path_trainingB+'/'+p)\n",
    "        attributes = df.columns[:-2]\n",
    "        df_clean = impute_missing_vals(df,attributes)\n",
    "        df_clean = df_clean.drop(['Unit1', 'Unit2', 'EtCO2'], axis=1)\n",
    "        concat_patientB.append(df_clean) # On rajoute le patient clean à la liste des patients.\n",
    "    df_allB = pd.concat(concat_patientB) # On créer un dataframe\n",
    "    df_allB.to_csv(data_path+'/allB') # On l'exporte en un CSV\n",
    "elif os.path.exists(data_path+'allA'): # Si le chemin vers le CSV existe, alors on le récupère.\n",
    "    df_allB = pd.read_csv(data_path+'/allB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b70808-a54e-4f7d-aac7-40747ca1b556",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc77440-5254-48a0-ad89-ede77889063d",
   "metadata": {},
   "source": [
    "<h3> Normalisation des données </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "580889cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler() # On instancie un objet pour MinMaxScaler et l'utiliser sur les données plus loin.\n",
    "list_normalized_A=[]\n",
    "for p in list_A:\n",
    "    df_minMax_scaled = min_max_scaler.fit_transform(p.copy()) # On transforme le Dataframe en une numpy Array mais en ayant en même temps normalisé les données.\n",
    "    list_normalized_A.append(df_minMax_scaled)\n",
    "\n",
    "list_normalized_B=[]\n",
    "for p in list_B:\n",
    "    df_minMax_scaled = min_max_scaler.fit_transform(p.copy()) # On effectue la même manipulation pour le training set B\n",
    "    list_normalized_B.append(df_minMax_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a62b035-8bd9-4cb8-9c73-39e600e99e1f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e5df2-9384-4c55-aa7f-dc975e7be14a",
   "metadata": {},
   "source": [
    "<h3> Entrainement et test du modèle </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c33f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

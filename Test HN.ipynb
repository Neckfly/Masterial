{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a73f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import trange\n",
    "\n",
    "from phn import EPOSolver, LinearScalarizationSolver\n",
    "\n",
    "from transformer.utils import *\n",
    "from transformer.Network import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ee41df",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{gpus}\" if torch.cuda.is_available() and not no_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175f19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb584ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle_points(K, min_angle=None, max_angle=None):\n",
    "    # generate evenly distributed preference vector\n",
    "    ang0 = 1e-6 if min_angle is None else min_angle\n",
    "    ang1 = np.pi / 2 - ang0 if max_angle is None else max_angle\n",
    "    angles = np.linspace(ang0, ang1, K, endpoint=True)\n",
    "    x = np.cos(angles)\n",
    "    y = np.sin(angles)\n",
    "    return np.c_[x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4080dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(hypernet, targetnet, loader, rays, device, lossWeight):\n",
    "    hypernet.eval()\n",
    "    results = {\"ray\" : [], \"loss\" : [], \"hv\" : []}#defaultdict(list)\n",
    "    \n",
    "    for ray in rays:\n",
    "        ray = torch.from_numpy(ray.astype(np.float32)).to(device)\n",
    "\n",
    "        ray /= ray.sum()\n",
    "\n",
    "        total = 0.0\n",
    "        full_losses = []\n",
    "        for batch in loader:\n",
    "            hypernet.zero_grad()\n",
    "\n",
    "            batch = (t.to(device) for t in batch)\n",
    "            X, Y = batch\n",
    "            bs = len(Y)\n",
    "\n",
    "            weights = hypernet(ray)\n",
    "            \n",
    "            transferParameters(targetnet, weights)\n",
    "            \n",
    "            pred = targetnet(X)\n",
    "\n",
    "            # loss\n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor([lossWeight]))\n",
    "            curr_losses = criterion(pred, Y)\n",
    "            \n",
    "            # metrics\n",
    "            ray = ray.squeeze(0)\n",
    "\n",
    "            # losses\n",
    "            full_losses.append(curr_losses.detach().cpu().numpy())\n",
    "            total += bs\n",
    "\n",
    "        results[\"ray\"].append(ray.cpu().numpy().tolist())\n",
    "        results[\"loss\"].append(np.array(full_losses).mean(0).tolist())\n",
    "\n",
    "    hv = get_performance_indicator(\n",
    "        \"hv\",\n",
    "        ref_point=np.ones(\n",
    "            7,\n",
    "        ),\n",
    "    )\n",
    "    results[\"hv\"] = hv.do(np.array(results[\"loss\"]))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead48bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, loaders, solver_type: str, hidden_dim: int, no_val_eval: bool, eval_every: int, alpha: float, \n",
    "          n_rays: int, epochs: int, lr: float, wd: float):\n",
    "    # ----\n",
    "    # Hypernetwork\n",
    "    # ----\n",
    "    hnet = TransformerHyper(ray_hidden_dim=hidden_dim, model=net)\n",
    "\n",
    "    net = net.to(device)\n",
    "    hnet = hnet.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(hnet.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # ------\n",
    "    # Solver\n",
    "    # ------\n",
    "    solvers = dict(ls=LinearScalarizationSolver, epo=EPOSolver)\n",
    "\n",
    "    solver_method = solvers[solver_type]\n",
    "    if solver_type == \"epo\":\n",
    "        nb_params = sum(p.numel() for p in hnet.parameters() if p.requires_grad)\n",
    "        solver = solver_method(n_tasks=2, n_params=nb_params)\n",
    "    else:\n",
    "        # ls\n",
    "        solver = solver_method(n_tasks=2)\n",
    "\n",
    "    # ----\n",
    "    # Data\n",
    "    # ----\n",
    "    \n",
    "    ### TODO: dataset to test_loader, val_loader, test_loader\n",
    "    train_loader = loaders['train_loader']\n",
    "    val_loader = loaders['val_loader']\n",
    "    test_loader = loaders['test_loader']\n",
    "    \n",
    "    ### TODO: DELETE UP\n",
    "\n",
    "    min_angle = 0.1\n",
    "    max_angle = np.pi / 2 - 0.1\n",
    "    test_rays = circle_points(n_rays, min_angle=min_angle, max_angle=max_angle)\n",
    "\n",
    "    # ----------\n",
    "    # Train loop\n",
    "    # ----------\n",
    "    last_eval = -1\n",
    "    epoch_iter = trange(epochs)\n",
    "\n",
    "    val_results = dict()\n",
    "    test_results = dict()\n",
    "\n",
    "    trainLossWeight = train_label_ts.count(0) / train_label_ts.count(1)\n",
    "    valLossWeight = val_label_ts.count(0) / val_label_ts.count(1)\n",
    "    testLossWeight = test_label_ts.count(0) / test_label_ts.count(1)\n",
    "    for epoch in epoch_iter:\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            hnet.train()\n",
    "            optimizer.zero_grad()\n",
    "            X, Y = batch\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            if alpha > 0:\n",
    "                ray = torch.from_numpy(\n",
    "                    np.random.dirichlet([alpha] * 2, 1).astype(np.float32).flatten()\n",
    "                ).to(device)\n",
    "            else:\n",
    "                alpha = torch.empty(\n",
    "                    1,\n",
    "                ).uniform_(0.0, 1.0)\n",
    "                ray = torch.tensor([alpha.item(), 1 - alpha.item()]).to(device)\n",
    "\n",
    "                \n",
    "            weights = hnet(ray)  \n",
    "       \n",
    "            #net.load_state_dict(weights)          \n",
    "            transferParameters(net, weights)\n",
    "            \n",
    "            pred = net(X)\n",
    "                  \n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor([trainLossWeight]))\n",
    "            losses = criterion(pred, Y)\n",
    "\n",
    "            ray = ray.squeeze(0)\n",
    "            loss = solver(losses, ray, list(hnet.parameters()), feat=X, label=Y, model=net, weights=weights)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            '''\n",
    "            epoch_iter.set_description(\n",
    "                f\"total weighted loss: {loss.item():.3f}\"\n",
    "                # f\", ray {ray.cpu().numpy().tolist()}\"\n",
    "            )\n",
    "            '''\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % eval_every == 0:\n",
    "            last_eval = epoch\n",
    "            if not no_val_eval:\n",
    "                epoch_results = evaluate(\n",
    "                    hypernet=hnet,\n",
    "                    targetnet=net,\n",
    "                    loader=val_loader,\n",
    "                    rays=test_rays,\n",
    "                    device=device,\n",
    "                    lossWeight=valLossWeight\n",
    "                )\n",
    "                #val_results[f\"epoch_{epoch + 1}\"] = epoch_results\n",
    "\n",
    "            test_epoch_results = evaluate(\n",
    "                hypernet=hnet,\n",
    "                targetnet=net,\n",
    "                loader=test_loader,\n",
    "                rays=test_rays,\n",
    "                device=device,\n",
    "                lossWeight=testLossWeight\n",
    "            )\n",
    "            #test_results[f\"epoch_{epoch + 1}\"] = test_epoch_results\n",
    "\n",
    "    if epoch != last_eval:\n",
    "        if not no_val_eval:\n",
    "            epoch_results = evaluate(\n",
    "                hypernet=hnet,\n",
    "                targetnet=net,\n",
    "                loader=val_loader,\n",
    "                rays=test_rays,\n",
    "                device=device,\n",
    "                lossWeight=valLossWeight\n",
    "            )\n",
    "            #val_results[f\"epoch_{epoch + 1}\"] = epoch_results\n",
    "\n",
    "        test_epoch_results = evaluate(\n",
    "            hypernet=hnet,\n",
    "            targetnet=net,\n",
    "            loader=test_loader,\n",
    "            rays=test_rays,\n",
    "            device=device,\n",
    "            lossWeight=testLossWeight\n",
    "        )\n",
    "        #test_results[f\"epoch_{epoch + 1}\"] = test_epoch_results\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab69c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.multimnist.data import Dataset\n",
    "\n",
    "from models import TransformerHyper, TargetTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6a6a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/pre_processed_dataset.csv')\n",
    "\n",
    "# Train = 0.6\n",
    "# Val = 0.1\n",
    "# Test = 0.3\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "samplelist = df[\"Patient_ID\"].unique()\n",
    "training_samp, split_samp = train_test_split(samplelist, train_size=0.6, test_size=0.4, random_state=5, shuffle=True)\n",
    "validation_samp, test_samp = train_test_split(samplelist, train_size=0.25, test_size=0.75, random_state=5, shuffle=True)\n",
    "    \n",
    "train_df = df[df['Patient_ID'].isin(training_samp)]\n",
    "val_df = df[df['Patient_ID'].isin(validation_samp)]\n",
    "test_df = df[df['Patient_ID'].isin(test_samp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22854269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDatasetPerPatient(dataset, window_size=6):\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for patientId in dataset['Patient_ID'].unique():\n",
    "        tmp_data = dataset[dataset['Patient_ID'] == patientId]\n",
    "        if(len(tmp_data) >= window_size):\n",
    "            data.append(tmp_data.drop(['Hour', 'Patient_ID', 'SepsisLabel'], axis=1).to_numpy())\n",
    "            label.append(tmp_data['SepsisLabel'].to_numpy())\n",
    "            \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d0c86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label = splitDatasetPerPatient(train_df)\n",
    "val_data, val_label = splitDatasetPerPatient(val_df)\n",
    "test_data, test_label = splitDatasetPerPatient(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "093c1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTimeSeriesDataloader(feat, label, window_size=6):\n",
    "    data_labels = []\n",
    "\n",
    "    # One patient per batch\n",
    "    data_loader = []\n",
    "\n",
    "    for i in range(len(feat)):\n",
    "        patient_data = feat[i]\n",
    "        labels = label[i]\n",
    "        X_data = []\n",
    "        Y_data = []\n",
    "\n",
    "        for j in range(len(patient_data) - (window_size - 1)):\n",
    "            X_data.append(patient_data[j:(j + window_size)])\n",
    "            Y_data.append([labels[(j + window_size - 1)]])\n",
    "            data_labels.append(labels[(j + window_size - 1)])\n",
    "\n",
    "        data_loader.append([torch.Tensor(X_data), torch.Tensor(Y_data)])\n",
    "        \n",
    "    return data_loader, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7165a3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neckf\\AppData\\Local\\Temp\\ipykernel_22700\\1662241016.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  data_loader.append([torch.Tensor(X_data), torch.Tensor(Y_data)])\n"
     ]
    }
   ],
   "source": [
    "train_loader, train_label_ts = toTimeSeriesDataloader(train_data, train_label)\n",
    "val_loader, val_label_ts = toTimeSeriesDataloader(val_data, val_label)\n",
    "test_loader, test_label_ts = toTimeSeriesDataloader(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a69ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {'train_loader': train_loader, 'val_loader': val_loader, 'test_loader': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "053327d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transferParameters(model, weights):\n",
    "    for name in weights:\n",
    "        obj = model._modules\n",
    "        #names = name.replace(\".\", \"/\").split('/')\n",
    "        names = name.split('.')\n",
    "        index = 0\n",
    "\n",
    "        while(index < len(names) - 1):\n",
    "\n",
    "            if (obj.__class__.__name__ == 'OrderedDict'): # Dict\n",
    "                obj = obj[names[index]]\n",
    "            elif (obj.__class__.__name__ == 'ModuleList'): # List\n",
    "                obj = obj[int(names[index])]\n",
    "            else: # Object\n",
    "                obj = getattr(obj, names[index])\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        #name = name.replace(\"/\", \".\")\n",
    "        if(names[index] == 'weight'):\n",
    "            del obj.weight           \n",
    "            obj.weight = weights[name]\n",
    "            #model.register_parameter(name.replace(\".\", \"/\"), nn.Parameter(weights[name]))\n",
    "        elif(names[index] == 'bias'):\n",
    "            del obj.bias\n",
    "            obj.bias = weights[name]\n",
    "            #model.register_parameter(name.replace(\".\", \"/\"), nn.Parameter(weights[name]))\n",
    "        elif(names[index] == 'pe'):\n",
    "            del obj.pe\n",
    "            obj.pe = weights[name]\n",
    "            #model.register_parameter(name.replace(\".\", \"/\"), nn.Parameter(weights[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f3d16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    }
   ],
   "source": [
    "net = Transformer(10, 2, 12, 6, 1, 1, 4, 4)\n",
    "\n",
    "trained_net = train(\n",
    "    net = net,\n",
    "    loaders=loaders,\n",
    "    solver_type=\"epo\",       \n",
    "    hidden_dim=10,  #100\n",
    "    eval_every=1,\n",
    "    no_val_eval=False,\n",
    "    alpha=0.2,\n",
    "    n_rays = 25,\n",
    "    epochs = 6,\n",
    "    lr = 1e-2,\n",
    "    wd = 0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48da13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.dirichlet(0.0 * 2, 1).astype(np.float32).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efbaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.dirichlet(0.5 * 2, 1).astype(np.float32).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.dirichlet(0.1 * 2, 1).astype(np.float32).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d524734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aee3036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = next(iter(train_loader))\n",
    "\n",
    "net = Transformer(10, 2, 12, 6, 1, 1, 4, 4)\n",
    "hnet = TransformerHyper(model=net)\n",
    "\n",
    "ray = torch.from_numpy(np.random.dirichlet([alpha] * 2, 1).astype(np.float32).flatten()).to(device)\n",
    "weights = hnet(ray)\n",
    "\n",
    "transferParameters(net, weights)\n",
    "pred = net(X)\n",
    "            \n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor([51.0]))\n",
    "losses = criterion(pred, Y)\n",
    "\n",
    "torch.autograd.grad(losses, list(hnet.parameters()), retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce28f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Transformer(10, 2, 12, 6, 1, 1, 4, 4)\n",
    "hnet = TransformerHyper(model=net)\n",
    "\n",
    "ray = torch.from_numpy(np.random.dirichlet([alpha] * 2, 1).astype(np.float32).flatten()).to(device)\n",
    "weights = hnet(ray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89871f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "transferParameters(net, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c39e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in net.named_parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df154e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in net.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22e762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in weights.items():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a440ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in net.state_dict().items():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ac919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43298db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import trange\n",
    "\n",
    "from phn import EPOSolver, LinearScalarizationSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0092c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{gpus}\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
    "\n",
    "lr = 1e-4\n",
    "wd = 0.0\n",
    "batch_size = 256\n",
    "n_rays = 25\n",
    "alpha = 0.2\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd3ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b5121df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle_points(K, min_angle=None, max_angle=None):\n",
    "    # generate evenly distributed preference vector\n",
    "    ang0 = 1e-6 if min_angle is None else min_angle\n",
    "    ang1 = np.pi / 2 - ang0 if max_angle is None else max_angle\n",
    "    angles = np.linspace(ang0, ang1, K, endpoint=True)\n",
    "    x = np.cos(angles)\n",
    "    y = np.sin(angles)\n",
    "    return np.c_[x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ff442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4080dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(hypernet, targetnet, loader, rays, device):\n",
    "    hypernet.eval()\n",
    "    loss1 = nn.CrossEntropyLoss()\n",
    "    loss2 = nn.CrossEntropyLoss()\n",
    "\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    for ray in rays:\n",
    "        total = 0.0\n",
    "        task1_correct, task2_correct = 0.0, 0.0\n",
    "        l1, l2 = 0.0, 0.0\n",
    "        ray = torch.from_numpy(ray.astype(np.float32)).to(device)\n",
    "        ray /= ray.sum()\n",
    "\n",
    "        for batch in loader:\n",
    "            hypernet.zero_grad()\n",
    "\n",
    "            batch = (t.to(device) for t in batch)\n",
    "            X, Y = batch\n",
    "            bs = len(Y)\n",
    "\n",
    "            weights = hypernet(ray)\n",
    "            logit1, logit2 = targetnet(X, weights)\n",
    "\n",
    "            # loss\n",
    "            curr_l1 = loss1(logit1, Y[:, 0])\n",
    "            curr_l2 = loss2(logit2, Y[:, 1])\n",
    "            l1 += curr_l1 * bs\n",
    "            l2 += curr_l2 * bs\n",
    "\n",
    "            # acc\n",
    "            pred1 = logit1.data.max(1)[1]  # first column has actual prob.\n",
    "            pred2 = logit2.data.max(1)[1]  # first column has actual prob.\n",
    "            task1_correct += pred1.eq(Y[:, 0]).sum()\n",
    "            task2_correct += pred2.eq(Y[:, 1]).sum()\n",
    "\n",
    "            total += bs\n",
    "\n",
    "        results[\"ray\"].append(ray.squeeze(0).cpu().numpy().tolist())\n",
    "        results[\"task1_acc\"].append(task1_correct.cpu().item() / total)\n",
    "        results[\"task2_acc\"].append(task2_correct.cpu().item() / total)\n",
    "        results[\"task1_loss\"].append(l1.cpu().item() / total)\n",
    "        results[\"task2_loss\"].append(l2.cpu().item() / total)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84a5d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loaders, solver_type: str, hidden_dim: int, no_val_eval: bool, eval_every: int, alpha: float):\n",
    "    # ----\n",
    "    # Nets\n",
    "    # ----\n",
    "    hnet = LeNetHyper([9, 5], ray_hidden_dim=hidden_dim)   #HN          LeNetHyper()\n",
    "    net = LeNetTarget([9, 5])                              #transformer  LeNetTarget()\n",
    "\n",
    "    hnet = hnet.to(device)\n",
    "    net = net.to(device)\n",
    "\n",
    "    # ---------\n",
    "    # Task loss\n",
    "    # ---------\n",
    "    loss1 = nn.CrossEntropyLoss()\n",
    "    loss2 = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(hnet.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # ------\n",
    "    # solver\n",
    "    # ------\n",
    "    solvers = dict(ls=LinearScalarizationSolver, epo=EPOSolver)\n",
    "\n",
    "    solver_method = solvers[solver_type]\n",
    "    if solver_type == \"epo\":\n",
    "        nb_params = sum(p.numel() for p in hnet.parameters() if p.requires_grad)\n",
    "        solver = solver_method(n_tasks=2, n_params=nb_params)\n",
    "    else:\n",
    "        # ls\n",
    "        solver = solver_method(n_tasks=2)\n",
    "\n",
    "    # ----\n",
    "    # data\n",
    "    # ----\n",
    "    \n",
    "    ### TODO: dataset to test_loader, val_loader, test_loader\n",
    "    train_loader = loaders['train_loader']\n",
    "    val_loader = loaders['val_loader']\n",
    "    test_loader = loaders['test_loader']\n",
    "    \n",
    "    ### TODO: DELETE UP\n",
    "\n",
    "    min_angle = 0.1\n",
    "    max_angle = np.pi / 2 - 0.1\n",
    "    test_rays = circle_points(n_rays, min_angle=min_angle, max_angle=max_angle)\n",
    "\n",
    "    # ----------\n",
    "    # Train loop\n",
    "    # ----------\n",
    "    last_eval = -1\n",
    "    epoch_iter = trange(epochs)\n",
    "\n",
    "    val_results = dict()\n",
    "    test_results = dict()\n",
    "\n",
    "    for epoch in epoch_iter:\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            hnet.train()\n",
    "            optimizer.zero_grad()\n",
    "            X, Y = batch\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            if alpha > 0:\n",
    "                ray = torch.from_numpy(\n",
    "                    np.random.dirichlet((alpha, alpha), 1).astype(np.float32).flatten()\n",
    "                ).to(device)\n",
    "            else:\n",
    "                alpha = torch.empty(\n",
    "                    1,\n",
    "                ).uniform_(0.0, 1.0)\n",
    "                ray = torch.tensor([alpha.item(), 1 - alpha.item()]).to(device)\n",
    "\n",
    "            weights = hnet(ray)\n",
    "            logit1, logit2 = net(X, weights)\n",
    "\n",
    "            l1 = loss1(logit1, Y[:, 0])\n",
    "            l2 = loss2(logit2, Y[:, 1])\n",
    "            losses = torch.stack((l1, l2))\n",
    "\n",
    "            ray = ray.squeeze(0)\n",
    "            loss = solver(losses, ray, list(hnet.parameters()), feat=X, label=Y, model=net)       #ajout des paramÃ¨tres feat, label et model\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            epoch_iter.set_description(\n",
    "                f\"total weighted loss: {loss.item():.3f}, loss 1: {l1.item():.3f}, loss 2: {l2.item():.3f}\"\n",
    "                # f\", ray {ray.cpu().numpy().tolist()}\"\n",
    "            )\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % eval_every == 0:\n",
    "            last_eval = epoch\n",
    "            if not no_val_eval:\n",
    "                epoch_results = evaluate(\n",
    "                    hypernet=hnet,\n",
    "                    targetnet=net,\n",
    "                    loader=val_loader,\n",
    "                    rays=test_rays,\n",
    "                    device=device,\n",
    "                )\n",
    "                val_results[f\"epoch_{epoch + 1}\"] = epoch_results\n",
    "\n",
    "            test_epoch_results = evaluate(\n",
    "                hypernet=hnet,\n",
    "                targetnet=net,\n",
    "                loader=test_loader,\n",
    "                rays=test_rays,\n",
    "                device=device,\n",
    "            )\n",
    "            test_results[f\"epoch_{epoch + 1}\"] = test_epoch_results\n",
    "\n",
    "    if epoch != last_eval:\n",
    "        if not no_val_eval:\n",
    "            epoch_results = evaluate(\n",
    "                hypernet=hnet,\n",
    "                targetnet=net,\n",
    "                loader=val_loader,\n",
    "                rays=test_rays,\n",
    "                device=device,\n",
    "            )\n",
    "            val_results[f\"epoch_{epoch + 1}\"] = epoch_results\n",
    "\n",
    "        test_epoch_results = evaluate(\n",
    "            hypernet=hnet,\n",
    "            targetnet=net,\n",
    "            loader=test_loader,\n",
    "            rays=test_rays,\n",
    "            device=device,\n",
    "        )\n",
    "        test_results[f\"epoch_{epoch + 1}\"] = test_epoch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0013fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.multimnist.data import Dataset\n",
    "\n",
    "from experiments.multimnist.models import (\n",
    "    LeNetHyper,\n",
    "    LeNetTarget,\n",
    "    ResnetHyper,\n",
    "    ResNetTarget,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8b736bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/pre_processed_dataset.csv')\n",
    "\n",
    "# Train = 0.6\n",
    "# Val = 0.1\n",
    "# Test = 0.3\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "samplelist = df[\"Patient_ID\"].unique()\n",
    "training_samp, split_samp = train_test_split(samplelist, train_size=0.6, test_size=0.4, random_state=5, shuffle=True)\n",
    "validation_samp, test_samp = train_test_split(samplelist, train_size=0.25, test_size=0.75, random_state=5, shuffle=True)\n",
    "    \n",
    "train_df = df[df['Patient_ID'].isin(training_samp)]\n",
    "val_df = df[df['Patient_ID'].isin(validation_samp)]\n",
    "test_df = df[df['Patient_ID'].isin(test_samp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b0a4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDatasetPerPatient(dataset, window_size=6):\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for patientId in dataset['Patient_ID'].unique():\n",
    "        tmp_data = dataset[dataset['Patient_ID'] == patientId]\n",
    "        if(len(tmp_data) >= window_size):\n",
    "            data.append(tmp_data.drop(['Hour', 'Patient_ID', 'SepsisLabel'], axis=1).to_numpy())\n",
    "            label.append(tmp_data['SepsisLabel'].to_numpy())\n",
    "            \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74b1b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label = splitDatasetPerPatient(train_df)\n",
    "val_data, val_label = splitDatasetPerPatient(val_df)\n",
    "test_data, test_label = splitDatasetPerPatient(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b85cfbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTimeSeriesDataloader(feat, label, window_size=6):\n",
    "    data_labels = []\n",
    "\n",
    "    # One patient per batch\n",
    "    data_loader = []\n",
    "\n",
    "    for i in range(len(feat)):\n",
    "        patient_data = feat[i]\n",
    "        labels = label[i]\n",
    "        X_data = []\n",
    "        Y_data = []\n",
    "\n",
    "        for j in range(len(patient_data) - (window_size - 1)):\n",
    "            X_data.append(patient_data[j:(j + window_size)])\n",
    "            Y_data.append([labels[(j + window_size - 1)]])\n",
    "            data_labels.append(labels[(j + window_size - 1)])\n",
    "\n",
    "        data_loader.append([torch.Tensor(X_data), torch.Tensor(Y_data)])\n",
    "        \n",
    "    return data_loader, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3debaf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, train_label_ts = toTimeSeriesDataloader(train_data, train_label)\n",
    "val_loader, val_label_ts = toTimeSeriesDataloader(val_data, val_label)\n",
    "test_loader, test_label_ts = toTimeSeriesDataloader(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bd8d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {'train_loader': train_loader, 'val_loader': val_loader, 'test_loader': test_loader,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe849e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4b7dbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [10, 1, 9, 9], expected input[1, 38, 6, 12] to have 1 channels, but got 38 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26192\\778726277.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mloaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msolver_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"epo\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mhidden_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0meval_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26192\\2148289106.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(loaders, solver_type, hidden_dim, no_val_eval, eval_every, alpha)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mlogit1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogit2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0ml1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Jupyter-workspaces\\Projet_AP_V2.0\\experiments\\multimnist\\models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, weights)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         x = F.conv2d(\n\u001b[0m\u001b[0;32m    127\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             weight=weights[\"conv0.weights\"].reshape(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [10, 1, 9, 9], expected input[1, 38, 6, 12] to have 1 channels, but got 38 channels instead"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    loaders=loaders,\n",
    "    solver_type=\"epo\",       \n",
    "    hidden_dim=100,\n",
    "    eval_every=10,\n",
    "    no_val_eval=False,\n",
    "    alpha=alpha\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d23018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
